{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a18ce5-c6e2-44fc-86cd-0b49f6cf2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "import os\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import bigquery\n",
    "import sys\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfd4b2-c9f7-496c-9db8-8455cce8690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the arguments and storing them in variables\n",
    "project_name=<<your_project_name>>\n",
    "dataset_name=<<your_bq_dataset_name>>\n",
    "bucket_name=<<your_code_bucket>>\n",
    "user_name=<<your_username_here>>\n",
    "\n",
    "bq_client = bigquery.Client(project=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40b203-a573-4230-84bd-50433df2f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a spark session\n",
    "spark = SparkSession.builder.appName('pyspark-retail-forecast-ml').config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00976ee-ffa2-48a5-8a37-54f1890cefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation\n",
    "\n",
    "#Reading the input datasets from GCS buckets\n",
    "aisles_df = spark.read.option(\"header\",True).csv(\"gs://\"+bucket_name+\"/retail_forecast_vertex_ai/01-datasets/aisles.csv\")\n",
    "departments_df= spark.read.option(\"header\",True).csv(\"gs://\"+bucket_name+\"/retail_forecast_vertex_ai/01-datasets/departments.csv\")\n",
    "orders_df = spark.read.option(\"header\",True).csv(\"gs://\"+bucket_name+\"/retail_forecast_vertex_ai/01-datasets/orders.csv\")\n",
    "products_df = spark.read.option(\"header\",True).csv(\"gs://\"+bucket_name+\"/retail_forecast_vertex_ai/01-datasets/products.csv\")\n",
    "train_df = spark.read.option(\"header\",True).csv(\"gs://\"+bucket_name+\"/retail_forecast_vertex_ai/01-datasets/order_products__train.csv\")\n",
    "prior_df = spark.read.option(\"header\",True).csv(\"gs://\"+bucket_name+\"/retail_forecast_vertex_ai/01-datasets/order_products__prior.csv\")\n",
    "\n",
    "aisles_df.show(2)\n",
    "departments_df.show(2)\n",
    "orders_df.show(2)\n",
    "products_df.show(2)\n",
    "train_df.show(2)\n",
    "prior_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722202dd-9622-4a7b-847e-c79046eba3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame with the orders and the products that have been purchased on prior orders (op)\n",
    "op=orders_df.join(prior_df,orders_df.order_id ==  prior_df.order_id,\"inner\")\n",
    "op.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448af0f-7104-40c1-85a7-d41c8d27837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate entries\n",
    "df_cols = op.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = op.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "op=df.drop(*cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c15936-6428-4aaa-8b5b-c8a4e17c9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distinct groups for each user, identify the highest order number in each group, save the new column to a DataFrame\n",
    "op = op.withColumn(\"order_number\", op[\"order_number\"].cast(IntegerType()))\n",
    "user=op.groupBy(\"user_id\")   .max(\"order_number\")   .select(col(\"user_id\"),col(\"max(order_number)\").alias(\"u_total_orders\"))\n",
    "user.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84be9b-a360-443a-94fa-b01e2a771ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How frequent a customer has reordered products\n",
    "op = op.withColumn(\"reordered\", op[\"reordered\"].cast(IntegerType()))\n",
    "u_reorder=op.groupBy(\"user_id\")   .max(\"reordered\")   .select(col(\"user_id\"),col(\"max(reordered)\").alias(\"u_reordered_ratio\"))\n",
    "u_reorder.show(2)\n",
    "\n",
    "user=user.join(u_reorder,user.user_id == u_reorder.user_id,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5af1d-c716-43ac-9def-d4c0e3248ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate entries\n",
    "df_cols = user.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = user.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "user=df.drop(*cols_to_remove)\n",
    "user.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb71df-b32f-40a1-8ca8-442b86a2dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrcting product and order information from customer order history\n",
    "op.createOrReplaceTempView(\"DATA\")\n",
    "prd=spark.sql(\"SELECT product_id,count(order_id) as p_total_purchases  FROM DATA GROUP BY product_id\")\n",
    "prd.show(2)\n",
    "\n",
    "p_reorder=op.groupBy(\"product_id\")   .avg(\"reordered\")   .select(col(\"product_id\"),col(\"avg(reordered)\").alias(\"p_reorder_ratio\"))\n",
    "p_reorder.show(2)\n",
    "\n",
    "prd=prd.join(p_reorder,prd.product_id == p_reorder.product_id,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbbd46-1449-43c9-913c-62611ae95063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate entries\n",
    "df_cols = prd.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = prd.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "prd=df.drop(*cols_to_remove)\n",
    "prd.show(2)\n",
    "prd.na.fill(value=0,subset=[\"p_reorder_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c62528-9e76-4d8e-be85-4bb037a27faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distinct groups for each combination of user and product, count orders, save the result for each user X product to a new DataFrame\n",
    "op.createOrReplaceTempView(\"DATA0\")\n",
    "upx=spark.sql(\"SELECT user_id,product_id,count(order_id) as uxp_total_bought  FROM DATA0 GROUP BY user_id,product_id\")\n",
    "upx.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd0440-0b56-4b90-a23d-d9f4110138b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How frequently a customer bought a product after its first purchase\n",
    "op.createOrReplaceTempView(\"DATA1\")\n",
    "times=spark.sql(\"SELECT user_id,product_id,count(order_id) as Times_Bought_N  FROM DATA1 GROUP BY user_id,product_id\")\n",
    "times.show(2)\n",
    "\n",
    "total_orders=op.groupBy(\"user_id\")   .max(\"order_number\")   .select(col(\"user_id\"),col(\"max(order_number)\").alias(\"total_orders\"))\n",
    "total_orders.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4872267-b802-45df-ba15-7949d12356e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The order number where the customer bought a product for first time ('first_order_number')\n",
    "op.createOrReplaceTempView(\"DATA2\")\n",
    "first_order_no=spark.sql(\"SELECT user_id,product_id,min(order_number) as first_order_number  FROM DATA2 GROUP BY user_id,product_id\")\n",
    "first_order_no.show(2)\n",
    "\n",
    "span=total_orders.join(first_order_no,total_orders.user_id == first_order_no.user_id,\"right\")\n",
    "span.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6976d4-630f-4c68-a1c1-4639800450a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each product get the total orders placed since its first order ('Order_Range_D')\n",
    "span=span.withColumn(\"Order_Range_D\", span.total_orders - span.first_order_number + 1)\n",
    "span.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe4344-a356-4d29-b8e8-ef1271718439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate entries\n",
    "df_cols = span.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = span.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "span=df.drop(*cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ec61f-3bc9-4339-8821-2fd790029fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final ratio \"uxp_reorder_ratio\"\n",
    "uxp_ratio=times.join(span,(times.user_id == span.user_id)&                      (times.product_id == span.product_id) ,\"left\")\n",
    "uxp_ratio=uxp_ratio.withColumn(\"uxp_reorder_ratio\", uxp_ratio.Times_Bought_N / uxp_ratio.Order_Range_D)\n",
    "cols=['Times_Bought_N', 'total_orders', 'first_order_number', 'Order_Range_D']\n",
    "uxp_ratio.drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74850245-d328-4118-9344-67cb7037a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Removing duplicate entries\n",
    "df_cols = uxp_ratio.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = uxp_ratio.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "\n",
    "uxp_ratio=df.drop(*cols_to_remove)\n",
    "uxp=upx.join(uxp_ratio,(upx.user_id == uxp_ratio.user_id)&                      (upx.product_id == uxp_ratio.product_id) ,\"left\")\n",
    "uxp_ratio.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011b1e9-2326-42ce-acd4-fd101b26a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate entries\n",
    "df_cols = uxp.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = uxp.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "uxp=df.drop(*cols_to_remove)\n",
    "\n",
    "data=uxp.join(user,uxp.user_id == user.user_id,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838152c3-4a41-48cc-8417-8dee4ef72ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate entries\n",
    "df_cols = data.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = data.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "\n",
    "data=df.drop(*cols_to_remove)\n",
    "data=data.join(prd,data.product_id == prd.product_id,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cb9fb-8d0d-446b-a25a-b74ec0d4e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate entries\n",
    "df_cols = data.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = data.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "data=df.drop(*cols_to_remove)\n",
    "\n",
    "data.show(2)\n",
    "\n",
    "\n",
    "\n",
    "orders_future=orders_df.filter((orders_df.eval_set == \"train\") | (orders_df.eval_set=='test'))\n",
    "orders_future=orders_future.select(orders_future['user_id'],orders_future['eval_set'],orders_future['order_id'])\n",
    "orders_future.show(2)\n",
    "\n",
    "data=data.join(orders_future,data.user_id == orders_future.user_id,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b71f9-ef2b-43dc-8fa7-9549ff0a7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Removing duplicate entries\n",
    "df_cols = data.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = data.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "data=df.drop(*cols_to_remove)\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fbe50-5139-4b9e-90af-98607e700ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "data_train=data.filter((data.eval_set == \"train\"))\n",
    "data_train.show(1)\n",
    "\n",
    "train1=train_df.select(train_df['product_id'],train_df['order_id'],train_df['reordered'])\n",
    "\n",
    "data_train=data_train.join(train1,(data_train.product_id == train1.product_id)&(data_train.order_id == train1.order_id),\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220e6f3-a0c1-4f50-ba1f-497bd457f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate entries\n",
    "df_cols = data_train.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = data_train.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "data_train=df.drop(*cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f880ded-e2da-4f21-bc4d-bd6d71502e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the previous merge, left a NaN value on reordered column means that the customers they haven't bought the product.\n",
    "# We change the value on them to 0.\n",
    "data_train.na.fill(value=0,subset=[\"reordered\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7b2ff-5059-44d0-8bd4-113592fecafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove all non-predictor variables\n",
    "cols_to_remove=['eval_set', 'order_id']\n",
    "data_train=data_train.drop(*cols_to_remove)\n",
    "data_train=data_train.fillna(value='0',subset=[\"reordered\"])\n",
    "data_train.groupBy('reordered').count().orderBy('count').show()\n",
    "data_train=data_train.limit(100000)\n",
    "data_train.groupBy('reordered').count().orderBy('count').show()\n",
    "\n",
    "data_train.printSchema()\n",
    "\n",
    "data_train = data_train.withColumn(\"product_id\", data_train[\"product_id\"].cast(IntegerType()))\n",
    "data_train = data_train.withColumn(\"user_id\", data_train[\"user_id\"].cast(IntegerType()))\n",
    "data_train = data_train.withColumn(\"reordered\", data_train[\"reordered\"].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454b5ea-cf43-429f-b20c-98c315cb5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the training dataset to BigQuery\n",
    "client = bigquery.Client()\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "table_id1 = dataset_name+'.'+user_name+'_train_data'\n",
    "\n",
    "\n",
    "d_train=data_train.toPandas()\n",
    "job1 = client.load_table_from_dataframe(\n",
    "    d_train, table_id1, job_config=job_config)\n",
    "job1.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a032477-05ef-4437-888f-0c43814f95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "# Keep only the future orders from customers who are labelled as test\n",
    "data_test=data.filter((data.eval_set == \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd151f-701f-46af-b278-136bc1047a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove all non-predictor variables\n",
    "cols_to_remove=['eval_set', 'order_id']\n",
    "data_test=data_test.drop(*cols_to_remove)\n",
    "\n",
    "data_test.printSchema()\n",
    "\n",
    "data_test = data_test.withColumn(\"product_id\", data_test[\"product_id\"].cast(IntegerType()))\n",
    "data_test = data_test.withColumn(\"user_id\", data_test[\"user_id\"].cast(IntegerType()))\n",
    "\n",
    "data_test.show(1)\n",
    "data_test=data_test.limit(40000)\n",
    "\n",
    "d_test=data_test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca390c9a-d622-4fcf-ae7b-42e72a58ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the evaluation dataset to BigQuery\n",
    "client = bigquery.Client()\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "table_id2=dataset_name+'.'+user_name+'_test_data'\n",
    "\n",
    "job1 = client.load_table_from_dataframe(\n",
    "    d_test, table_id2, job_config=job_config)\n",
    "job1.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cafdf5-11d4-4a3b-94f8-82855f7cd02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to summarize data characteristics\n",
    "def summarize_data(df):\n",
    "    print(\"\\nOverview\")\n",
    "    print(df.head())\n",
    "    print(\"\\nSummary\")\n",
    "    print(df.describe())\n",
    "    print(\"\\nNull Values\")\n",
    "    print(\"\\nCount\")\n",
    "    print(df.count())\n",
    "\n",
    "summarize_data(aisles_df)\n",
    "summarize_data(orders_df)\n",
    "summarize_data(departments_df)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4cda8-8e19-4853-84ac-3835c9d6a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modeling\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark-retail-forecast-ml').config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').getOrCreate()\n",
    "\n",
    "train_df = spark.read \\\n",
    "  .format('bigquery') \\\n",
    "  .load(project_name+'.'+dataset_name+'.'+user_name+'_train_data')\n",
    "\n",
    "train_df.show(2)\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b4bb3-049d-467c-bea3-865025e4e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the key features required for the Random Forest Model\n",
    "features=['uxp_total_bought',\n",
    " 'Times_Bought_N',\n",
    " 'total_orders',\n",
    " 'first_order_number',\n",
    " 'Order_Range_D',\n",
    " 'uxp_reorder_ratio',\n",
    " 'u_total_orders',\n",
    " 'u_reordered_ratio',\n",
    " 'p_total_purchases',\n",
    " 'p_reorder_ratio'   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28839ae-117f-4941-8891-6fb1dfe7655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining and transforming the Vector Assembler on the input data\n",
    "assembler = VectorAssembler(inputCols=features,outputCol='features')\n",
    "va_df=assembler.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed01ea-6b9d-40ad-80e2-b45873336a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining and transforming the String Indexer on the input data\n",
    "indexer = StringIndexer(inputCol = 'reordered', outputCol = 'label')\n",
    "i_df = indexer.fit(va_df).transform(va_df)\n",
    "\n",
    "i_df.printSchema()\n",
    "i_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90a76a-aa95-4deb-8e64-71a55b424bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the input data into train and test datasets\n",
    "splits = i_df.randomSplit([0.6,0.4],1)\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "train_df.count(), test_df.count(), i_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee127c00-6f00-45df-a7e1-c1b0b9b37a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining and training the Random Forest Classification Model\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa776bbd-16ce-4345-abdf-dbd3c4555dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the Random Forest Model on test data\n",
    "predictions = rfModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313b144-26f8-4544-b487-1f301699c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model performance metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa88214-aa12-40ca-8172-c05b412f1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the model output to BigQuery\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)\n",
    "predictions.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_predictions_data') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8e815-438a-4749-b7a5-04eaea76ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model to Google Cloud Storage\n",
    "rfModel.write().overwrite().save('gs://'+bucket_name+'/model/rf_model.model')\n",
    "spark.stop()\n",
    "\n",
    "print('Modeling Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e164-c49c-4b85-962f-a8269c9016b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark-retail-forecast-ml').config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679387e-fa2c-4e38-b554-d5bdbef9171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the evaluation data from BigQuery\n",
    "test_df = spark.read \\\n",
    "  .format('bigquery') \\\n",
    "  .load(project_name+'.'+dataset_name+'.'+user_name+'_test_data')\n",
    "\n",
    "bq_client = bigquery.Client(project=project_name)\n",
    "test_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25460621-2b9b-4e92-9f5a-8d1c07cdb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Random Forest Classifier Model from GCS:\n",
    "model_1 = RandomForestClassificationModel.load(os.path.join('gs://'+bucket_name+'/model/rf_model.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9834c6-c7d0-4eb0-8e91-8fa765e925ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical columns that are to be passed in Vector Assembler\n",
    "features=['uxp_total_bought',\n",
    " 'Times_Bought_N',\n",
    " 'total_orders',\n",
    " 'first_order_number',\n",
    " 'Order_Range_D',\n",
    " 'uxp_reorder_ratio',\n",
    " 'u_total_orders',\n",
    " 'u_reordered_ratio',\n",
    " 'p_total_purchases',\n",
    " 'p_reorder_ratio'   ]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features,outputCol='features')\n",
    "va_df=assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae01b54-ba04-4b36-b5e0-06ef8630e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the model on the evaluation dataset\n",
    "test=model_1.transform(va_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96012c65-972a-4833-8efb-19372ef7adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Denormalizing the model evaluation output dataset\n",
    "test1=test.select(['user_id','prediction'])\n",
    "test2=test1.join(test_df,test1.user_id== test_df.user_id,'inner')\n",
    "test2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62fbb7f-6674-4167-abd7-3302562c9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Duplicate columns\n",
    "df_cols = test2.columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "df = test2.toDF(*df_cols)\n",
    "cols_to_remove = [c for c in df_cols if '_duplicated' in c]\n",
    "test2=df.drop(*cols_to_remove)\n",
    "test2=test2.select(['user_id','product_id','prediction'])\n",
    "test2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6f75f-7c95-42b8-862d-86a73cb2faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writting the output to BigQuery\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)\n",
    "test2.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_eval_output') \\\n",
    ".save()\n",
    "\n",
    "print('Job Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8204098-6e82-4e89-a5a2-aec99a2c7bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c295b3d-7516-4bfa-baa8-7b68fafb3597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cf46d-ed53-4d1d-840b-bc786429e1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad292d-e68a-42bc-a9e8-49dfc9146cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] * (Local)",
   "language": "python",
   "name": "local-conda-root-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
